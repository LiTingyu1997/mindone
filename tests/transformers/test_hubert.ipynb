{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T16:31:12.006821Z",
     "start_time": "2024-08-12T16:31:04.490779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from convert_to_safetensors import convert_bin_to_safetensors\n",
    "input_bin_path = \"/home/pingqi/.cache/huggingface/hub/models--facebook--hubert-large-ls960-ft/snapshots/ece5fabbf034c1073acae96d5401b25be96709d8/pytorch_model.bin\"\n",
    "output_sf_path = \"/home/pingqi/.cache/huggingface/hub/models--facebook--hubert-large-ls960-ft/snapshots/ece5fabbf034c1073acae96d5401b25be96709d8/model.safetensors\"\n",
    "model_path = \"/home/pingqi/.cache/huggingface/hub/models--facebook--hubert-large-ls960-ft/snapshots/ece5fabbf034c1073acae96d5401b25be96709d8/\"\n",
    "convert_bin_to_safetensors(input_bin_path, output_sf_path)"
   ],
   "id": "caf77dbee1327f74",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing HubertForCTC: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForCTC were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22.67686"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T16:38:11.161008Z",
     "start_time": "2024-08-12T16:37:59.609961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoProcessor, HubertForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "model = HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "    \n",
    "# audio file is decoded on the fly\n",
    "inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# transcribe speech\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "print(transcription[0])\n",
    "\n",
    "inputs[\"labels\"] = processor(text=dataset[0][\"text\"], return_tensors=\"pt\").input_ids\n",
    "\n",
    "# compute loss\n",
    "pt_loss = model(**inputs).loss\n",
    "print(round(pt_loss.item(), 5))"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing HubertForCTC: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForCTC were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
      "22.67686\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T16:44:04.501784Z",
     "start_time": "2024-08-12T16:43:51.714643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoProcessor\n",
    "from mindone.transformers.models.hubert import HubertForCTC as ms_HubertForCTC\n",
    "from datasets import load_dataset\n",
    "import mindspore\n",
    "from mindspore import Tensor\n",
    "mindspore.set_context(mode=1, pynative_synchronize=True)\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "model_path = \"/home/pingqi/.cache/huggingface/hub/models--facebook--hubert-large-ls960-ft/snapshots/ece5fabbf034c1073acae96d5401b25be96709d8\"\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "model = ms_HubertForCTC.from_pretrained(model_path)\n",
    "\n",
    "# audio file is decoded on the fly\n",
    "inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"np\")\n",
    "for k, v in inputs.items():\n",
    "    inputs[k] = Tensor(v)\n",
    "\n",
    "logits = model(**inputs).logits\n",
    "predicted_ids = mindspore.ops.argmax(logits, dim=-1)\n",
    "\n",
    "# transcribe speech\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "print(transcription[0])\n",
    "\n",
    "inputs[\"labels\"] = Tensor(processor(text=dataset[0][\"text\"], return_tensors=\"np\").input_ids)\n",
    "    \n",
    "# compute loss\n",
    "ms_pynative_loss = model(**inputs).loss\n",
    "print(round(ms_pynative_loss.item(), 5))"
   ],
   "id": "3551803973a07f3c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.284 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "[WARNING] ME(2022:140601435662144,MainProcess):2024-08-13-00:44:00.137.190 [mindspore/train/serialization.py:1560] For 'load_param_into_net', 1 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.\n",
      "[WARNING] ME(2022:140601435662144,MainProcess):2024-08-13-00:44:00.137.995 [mindspore/train/serialization.py:1564] ['hubert.encoder.pos_conv_embed.conv.weight'] are not loaded.\n",
      "Some weights of HubertForCTC were not initialized from the model checkpoint at /home/pingqi/.cache/huggingface/hub/models--facebook--hubert-large-ls960-ft/snapshots/ece5fabbf034c1073acae96d5401b25be96709d8 and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
      "22.67684\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
